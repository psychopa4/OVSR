function: 'train' # train, test, get_complexity

path:
  base: './'
  train: './data/filelist_train.txt'   # use .txt file, or the folder of your data like /xxx/data
  eval: './data/filelist_val.txt'
  test: '/xxx/data'
  checkpoint: 'checkpoint'
  eval_result: 'eval'

gpus: [0, 1]
data_kind: 'single'    # single for hr only, double for lr and hr

train:
  resume: -999           # -999 to load the latest checkpoint
  num_frame: 9           # number of frames for training
  sub_frame: 1           # number of frames at the beginning and the end not used for computing the loss during training
  batch_size: 16
  in_size: 64            # patch_size of LR frames during training
  init_lr: 1.e-3         # initial learning rate, and you may lower it
  final_lr: 1.e-4        # decays to the final learning rate after {epoch_decay} epochs
  epoch_decay: 240       # number of epochs from init_lr to final_lr
  loss: 'cha_loss'       # cha_loss, MSELoss, L1Loss
  alpha: 0.01            # hyper-parameter to adjust the weight of precursor in loss, 0.01 and 0.1 for global and local should be fine
  num_epochs: 800        # max number of epochs for training
  num_workers: 4
  iter_per_epoch: 500    # iterations per epoch 
  display_iter: 20       # iterations to print info
  epoch: 0               # current eopch, updated during training

eval:
  num_workers: 4
  batch_size: 1

test:
  save_name: 'govsr_8+4_80'       # name to save test results

model:
  file: 'ovsr'                    # name of model file (models/ovsr.py)
  name: 'govsr_8+4_80'            # name to save/load checkpoint and evaluation, change it for different settings
  kind: 'global'                  # local or global
  num_frame: 3                    # we adopt one frame from the past, present, and future respectively
  scale: 4
  num_pb: 8                       # number of residual blocks in the precursor
  num_sb: 4                       # number of residual blocks in the successor
  basic_filter: 80                # number of filters for conv layer